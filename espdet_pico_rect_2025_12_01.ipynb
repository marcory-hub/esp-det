{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPvSCDnCcsrQfTJXPjIIIOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcory-hub/esp-det/blob/main/espdet_pico_rect_2025_12_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESP detection"
      ],
      "metadata": {
        "id": "0pxEvLE1m4z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESP-Detection Training and Quantization for ESP32\n",
        "\n",
        "This notebook provides a complete workflow for training, exporting, and quantizing object detection models using **ESP-Detection**, a framework based on Ultralytics YOLO11n optimized for efficient deployment on ESP AI chips (ESP32-P4 and ESP32-S3).\n",
        "\n",
        "## Overview\n",
        "\n",
        "ESP-Detection enables deployment of lightweight object detection models on resource-constrained ESP32 microcontrollers. This notebook automates the entire pipeline:\n",
        "\n",
        "1. **Model Training**: Train ESPDet-Pico models on custom datasets\n",
        "2. **Model Export**: Convert trained PyTorch models to ONNX format\n",
        "3. **Quantization**: Generate optimized `.espdl` models for ESP32 deployment\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- **Google Colab with GPU** (T4 or better recommended)\n",
        "- **Python 3.8** virtual environment (required for NumPy 1.24.4 compatibility)\n",
        "- **Dataset**: YOLO-format dataset uploaded to Google Drive\n",
        "\n",
        "## Why Python 3.8?\n",
        "\n",
        "The ESP-Detection repository requires NumPy 1.24.4, which is only compatible with Python 3.8-3.11. Python 3.12+ uses NumPy 2.x by default, causing dependency conflicts. This notebook automatically sets up a Python 3.8 virtual environment to ensure compatibility.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Setup**: Install Python 3.8, create virtual environment, and clone ESP-Detection\n",
        "2. **Dataset Preparation**: Mount Google Drive and prepare calibration data\n",
        "3. **Training**: Train ESPDet-Pico model on your custom dataset\n",
        "4. **Export & Quantization**: Convert to ONNX and quantize for ESP32-S3/P4\n",
        "5. **Download**: Get the final `.espdl` model file for deployment\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "- **ESPDet-Pico**: Lightweight detection model (~360K parameters)\n",
        "- **Input Size**: 288√ó288 pixels (Rect: False) or 160x288 pixels (Rect: True)\n",
        "- **Output**: Optimized `.espdl` format for ESP-DL inference\n",
        "\n",
        "---\n",
        "\n",
        "**Repository**: [esp-detection](https://github.com/espressif/esp-detection)  \n",
        "**Documentation**: See ESP-Detection GitHub repository for deployment instructions"
      ],
      "metadata": {
        "id": "elRkP3fxlmGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Make sure images and labels from your dataset have this folder structure with these exact names. And add `data.yaml` to main folder.\n",
        "\n",
        "```\n",
        "üóÇÔ∏è dataset\n",
        "  üóÇÔ∏è train\n",
        "    üóÇÔ∏è images\n",
        "    üóÇÔ∏è labels\n",
        "  üóÇÔ∏è valid\n",
        "    üóÇÔ∏è images\n",
        "    üóÇÔ∏è labels\n",
        "  data.yaml\n",
        "```\n",
        "\n",
        "2. Zip the dataset folder to a file names `dataset.zip` On mac use `zip -r dataset.zip . -x \"*.DS_Store\" \"__MACOSX/*\" \".Trashes/*\" \".Spotlight-V100/*\" \".TemporaryItems/*\"` to exclude hidden files, such as finderfiles, from the zipped file.\n",
        "\n",
        "3. Make in /content/drive/MyDrive the folder `yolo`. Copy the `dataset.zip` file to this folder, it is needed to make a callibration image set and your yolo model, fe `best.pt` to this folder. For the model you can use a custom name and adjust it in the options below.\n",
        "\n",
        "4. Copy the `dataset.zip` file to the folder /content/drive/MyDrive/yolo.\n"
      ],
      "metadata": {
        "id": "xrZ3Ldzzl8fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU, mount google drive, copy dataset, unzip and create subset for calibration\n"
      ],
      "metadata": {
        "id": "vbW97JpeUksw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BeEdLVKUYZ5"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "yJU-GEn1ZPTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy zipped dataset to colab and unzip\n",
        "!scp '/content/drive/MyDrive/yolo/dataset.zip' '/content/dataset.zip'\n",
        "!unzip '/content/dataset.zip' -d '/content/'"
      ],
      "metadata": {
        "id": "9GUCt50zZVlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install envirionment and esp-detection"
      ],
      "metadata": {
        "id": "fB3zr15dFD97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Python 3.8\n",
        "!sudo apt update\n",
        "!sudo apt install -y python3.8 python3.8-venv python3.8-dev\n",
        "\n",
        "# Create virtual environment (Python 3.8)\n",
        "!python3.8 -m venv /content/env_esp\n",
        "\n",
        "# Install pip for Python 3.8\n",
        "!wget https://bootstrap.pypa.io/pip/3.8/get-pip.py\n",
        "!/content/env_esp/bin/python get-pip.py\n",
        "\n",
        "# Install NumPy 1.24.4 inside the venv\n",
        "!/content/env_esp/bin/pip install numpy==1.24.4\n",
        "\n",
        "# Install PyTorch for Python 3.8\n",
        "!/content/env_esp/bin/pip install torch==2.2.0 torchvision==0.17.0 torchaudio\n",
        "\n",
        "# Clone esp-detection and install requirements inside venv\n",
        "!git clone --recursive https://github.com/espressif/esp-detection.git\n",
        "!/content/env_esp/bin/pip install -r /content/esp-detection/requirements.txt"
      ],
      "metadata": {
        "id": "cLhAH8mnUyUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert yolo yaml to espdet yaml"
      ],
      "metadata": {
        "id": "-iTeVxU5cAIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "root = \"/content\"\n",
        "esp_root = \"/content/esp-detection\"\n",
        "data_yaml_in = f\"{root}/data.yaml\"\n",
        "dataset_dir = f\"{esp_root}/cfg/datasets\"\n",
        "\n",
        "# Load original data.yaml\n",
        "with open(data_yaml_in, 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "# Ensure correct folder exists inside esp-detection\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Create esp-detection dataset format\n",
        "esp_data = {\n",
        "    'path': root,            # /content\n",
        "    'train': 'train/images', # relative to path\n",
        "    'val': 'valid/images',\n",
        "    'nc': data['nc'],\n",
        "    'names': {i: name for i, name in enumerate(data['names'])}\n",
        "}\n",
        "\n",
        "dataset_yaml_path = f\"{dataset_dir}/dataset.yaml\"\n",
        "\n",
        "# Write YAML in /content/esp-detection/cfg/datasets/\n",
        "with open(dataset_yaml_path, 'w') as f:\n",
        "    yaml.dump(esp_data, f, default_flow_style=False)\n",
        "\n",
        "print(\"Created:\", dataset_yaml_path)\n",
        "print(f\"Classes: {esp_data['names']}\")\n"
      ],
      "metadata": {
        "id": "lWiQRO_ycEAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make calibrationset\n",
        "Default 500 images per class, 15% null images\n",
        "- Imgsz: 288"
      ],
      "metadata": {
        "id": "Vsu6n4FDcFHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import yaml\n",
        "from PIL import Image\n",
        "\n",
        "# Parameters\n",
        "num_images_per_class = 500\n",
        "null_images_ratio = 0.15  # 15% of total calibration set\n",
        "dataset_dir = \"/content\"\n",
        "calibration_dir = \"/content/esp-detection/deploy/calib_data\"\n",
        "imgsz = (288, 288)\n",
        "\n",
        "# Read class names from data.yaml\n",
        "try:\n",
        "    with open('/content/data.yaml', 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "\n",
        "    if data is None or 'names' not in data:\n",
        "        raise ValueError(\"Invalid data.yaml\")\n",
        "\n",
        "    class_names = data['names']\n",
        "\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error reading data.yaml: {e}\")\n",
        "\n",
        "# Create calibration directory\n",
        "os.makedirs(calibration_dir, exist_ok=True)\n",
        "\n",
        "train_images_dir = os.path.join(dataset_dir, \"train\", \"images\")\n",
        "train_labels_dir = os.path.join(dataset_dir, \"train\", \"labels\")\n",
        "\n",
        "if not os.path.exists(train_images_dir):\n",
        "    raise FileNotFoundError(f\"Training images directory not found at {train_images_dir}\")\n",
        "\n",
        "total_copied = 0\n",
        "null_images = []\n",
        "errors = []\n",
        "\n",
        "# Collect null images (empty label files)\n",
        "print(\"Collecting null images (empty label files)...\")\n",
        "if os.path.exists(train_labels_dir):\n",
        "    for label_file in os.listdir(train_labels_dir):\n",
        "        if not label_file.endswith('.txt'):\n",
        "            continue\n",
        "\n",
        "        label_path = os.path.join(train_labels_dir, label_file)\n",
        "        image_name = label_file.replace('.txt', '')\n",
        "\n",
        "        if not image_name:\n",
        "            continue\n",
        "\n",
        "        # Check if label file is empty (null image)\n",
        "        try:\n",
        "            with open(label_path, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                if not content:  # Empty = null image\n",
        "                    # Find corresponding image\n",
        "                    for ext in ['.jpg', '.jpeg', '.png']:\n",
        "                        image_path = os.path.join(train_images_dir, image_name + ext)\n",
        "                        if os.path.exists(image_path):\n",
        "                            null_images.append(image_name + ext)\n",
        "                            break\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Error checking {label_path}: {e}\")\n",
        "\n",
        "print(f\"Found {len(null_images)} null images\")\n",
        "\n",
        "# Process each class\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    if not class_name:\n",
        "        print(f\"‚ö†Ô∏è  Skipping class {class_idx}: name is empty\")\n",
        "        continue\n",
        "\n",
        "    class_images = []\n",
        "\n",
        "    if os.path.exists(train_labels_dir):\n",
        "        for label_file in os.listdir(train_labels_dir):\n",
        "            if not label_file.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            label_path = os.path.join(train_labels_dir, label_file)\n",
        "            image_name = label_file.replace('.txt', '')\n",
        "\n",
        "            if not image_name:\n",
        "                continue\n",
        "\n",
        "            # Skip null images (empty label files) - handled separately\n",
        "            try:\n",
        "                with open(label_path, 'r') as f:\n",
        "                    if not f.read().strip():  # Empty = skip\n",
        "                        continue\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            for ext in ['.jpg', '.jpeg', '.png']:\n",
        "                image_path = os.path.join(train_images_dir, image_name + ext)\n",
        "\n",
        "                if os.path.exists(image_path):\n",
        "                    try:\n",
        "                        with open(label_path, 'r') as f:\n",
        "                            labels = f.readlines()\n",
        "\n",
        "                            for label in labels:\n",
        "                                if not label or not label.strip():\n",
        "                                    continue\n",
        "\n",
        "                                parts = label.strip().split()\n",
        "\n",
        "                                if len(parts) >= 5:\n",
        "                                    try:\n",
        "                                        if int(parts[0]) == class_idx:\n",
        "                                            class_images.append(image_name + ext)\n",
        "                                            break\n",
        "                                    except (ValueError, IndexError):\n",
        "                                        continue\n",
        "                    except Exception as e:\n",
        "                        errors.append(f\"Error reading {label_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    break\n",
        "\n",
        "    num_to_select = min(num_images_per_class, len(class_images))\n",
        "\n",
        "    if num_to_select == 0:\n",
        "        print(f\"‚ö†Ô∏è  Class {class_idx} ({class_name}): No images found\")\n",
        "        continue\n",
        "\n",
        "    selected = random.sample(class_images, num_to_select)\n",
        "\n",
        "    for img_name in selected:\n",
        "        src_path = os.path.join(train_images_dir, img_name)\n",
        "        dst_path = os.path.join(calibration_dir, img_name)\n",
        "\n",
        "        try:\n",
        "            img = Image.open(src_path)\n",
        "            img_resized = img.resize(imgsz[::-1], Image.Resampling.LANCZOS)\n",
        "            img_resized.save(dst_path)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Error processing {img_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Class {class_idx} ({class_name}): {len(selected)} images\")\n",
        "    total_copied += len(selected)\n",
        "\n",
        "# Add null images - 15% of total calibration set\n",
        "if null_images:\n",
        "    total_class_images = total_copied\n",
        "    num_null = int(total_class_images * null_images_ratio / (1 - null_images_ratio))\n",
        "    num_null = min(num_null, len(null_images))\n",
        "\n",
        "    if num_null > 0:\n",
        "        selected_null = random.sample(null_images, num_null)\n",
        "\n",
        "        print(f\"\\nProcessing null images (empty labels)...\")\n",
        "        for img_name in selected_null:\n",
        "            src_path = os.path.join(train_images_dir, img_name)\n",
        "            dst_path = os.path.join(calibration_dir, img_name)\n",
        "\n",
        "            try:\n",
        "                img = Image.open(src_path)\n",
        "                img_resized = img.resize(imgsz[::-1], Image.Resampling.LANCZOS)\n",
        "                img_resized.save(dst_path)\n",
        "                total_copied += 1\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Error processing null image {img_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        actual_ratio = (len(selected_null) / total_copied) * 100\n",
        "        print(f\"Null images: {len(selected_null)} images ({actual_ratio:.1f}% of total calibration set)\")\n",
        "\n",
        "print(f\"\\nCalibration data: {total_copied} images in {calibration_dir}\")\n",
        "\n",
        "if errors:\n",
        "    print(f\"\\n‚ö†Ô∏è  {len(errors)} errors (first 5):\")\n",
        "    for error in errors[:5]:\n",
        "        print(f\"  - {error}\")"
      ],
      "metadata": {
        "id": "U-42p-Xpe8i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train espdet-pico model\n",
        "Default 300 epochs (change to 10 for test purpose)\n",
        "- Imgsz: 288\n",
        "- Rect: False\n"
      ],
      "metadata": {
        "id": "oEnJiyNPCxwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Working directory\n",
        "os.chdir(\"/content/esp-detection\")\n",
        "\n",
        "dataset_yaml = \"cfg/datasets/dataset.yaml\"\n",
        "imgsz = 288\n",
        "epochs = 300\n",
        "\n",
        "venv_python = \"/content/env_esp/bin/python\"\n",
        "\n",
        "# Create training script\n",
        "train_script = f\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from train import Train\n",
        "\n",
        "# Ensure correct directory\n",
        "os.chdir(\"/content/esp-detection\")\n",
        "\n",
        "# Clear memory before training\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Run training\n",
        "results = Train(\n",
        "    dataset=\"{dataset_yaml}\",\n",
        "    imgsz={imgsz},\n",
        "    epochs={epochs},\n",
        "    rect=False,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "# Clear memory after training\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\"\"\"\n",
        "\n",
        "# Save script\n",
        "train_script_path = \"/content/esp-detection/train_colab.py\"\n",
        "with open(train_script_path, \"w\") as f:\n",
        "    f.write(train_script)\n",
        "\n",
        "print(f\"Training script written to {train_script_path}\")\n",
        "\n",
        "# Run under Python 3.8 venv\n",
        "!{venv_python} {train_script_path}\n"
      ],
      "metadata": {
        "id": "bt1MiFi9XAP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export to onnx and quantize for esp32-s3\n",
        "Remember to check save_dir if more than one training was done\n",
        "- Input size: 288"
      ],
      "metadata": {
        "id": "QKtAc4tCC4aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# Paths\n",
        "save_dir = \"/content/esp-detection/runs/detect/train\"  # Update with your actual training run dir\n",
        "model_path = os.path.join(save_dir, \"weights/best.pt\")\n",
        "onnx_path = model_path.replace(\".pt\", \".onnx\")\n",
        "espdl_path = \"espdet_pico_288_e300.espdl\"\n",
        "\n",
        "# Verify trained model exists\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"Model not found at {model_path}. Check training completed successfully.\")\n",
        "\n",
        "# Write the Python 3.8 command to run export + quantization in venv\n",
        "export_script = f\"\"\"\n",
        "import torch\n",
        "from deploy.export import Export\n",
        "from deploy.quantize import quant_espdet\n",
        "\n",
        "# Export to ONNX\n",
        "Export(\n",
        "    model_path=\"{model_path}\",\n",
        "    input_size=[288, 288],\n",
        ")\n",
        "\n",
        "# Quantize for ESP32-S3\n",
        "quant_espdet(\n",
        "    onnx_path=\"{onnx_path}\",\n",
        "    target=\"esp32s3\",\n",
        "    num_of_bits=8,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    batchsz=32,\n",
        "    imgsz=[288, 288],\n",
        "    calib_dir=\"deploy/calib_data\",\n",
        "    espdl_model_path=\"{espdl_path}\",\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Save export script\n",
        "with open(\"/content/esp-detection/export_quant.py\", \"w\") as f:\n",
        "    f.write(export_script)\n",
        "\n",
        "# Run the export + quantization using Python 3.8 venv\n",
        "!echo \"Running export + quantization under Python 3.8 venv...\"\n",
        "!/content/env_esp/bin/python /content/esp-detection/export_quant.py\n",
        "\n",
        "# Verify output\n",
        "if os.path.exists(espdl_path):\n",
        "    print(f\"Quantized model saved: {espdl_path}\")\n",
        "    print(f\"File size: {os.path.getsize(espdl_path) / 1024 / 1024:.2f} MB\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Quantization failed. .espdl file not found.\")\n",
        "\n",
        "\n",
        "\n",
        "# Download the file\n",
        "file_path = f\"/content/esp-detection/{espdl_path}\"\n",
        "\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "id": "qsCvLTt3f1DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zip and Download to Google Drive and Local Computer\n",
        "Remember to check save_dir if more than one training was done"
      ],
      "metadata": {
        "id": "3mhhNdGJYgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training results and model files\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# Paths - Update if needed\n",
        "save_dir = \"/content/esp-detection/runs/detect/train/\"\n",
        "model_path = os.path.join(save_dir, \"weights/best.pt\")\n",
        "onnx_path = model_path.replace(\".pt\", \".onnx\")\n",
        "espdl_path = \"/content/esp-detection/espdet_pico_288_e300.espdl\"\n",
        "\n",
        "# Create timestamped folder name\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H%M\")\n",
        "drive_folder = f\"/content/drive/MyDrive/espdet-{timestamp}\"\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# 1. Zip entire train folder for local download\n",
        "train_zip = f\"/content/esp-detection/train-{timestamp}.zip\"\n",
        "if os.path.exists(save_dir):\n",
        "    print(\"Creating zip with training results...\")\n",
        "    shutil.make_archive(train_zip.replace('.zip', ''), 'zip', save_dir)\n",
        "    if os.path.exists(train_zip):\n",
        "        zip_size = os.path.getsize(train_zip) / 1024 / 1024\n",
        "        print(f\"‚úì Training results zip: {os.path.basename(train_zip)} ({zip_size:.2f} MB)\")\n",
        "        print(\"Downloading training results to local computer...\")\n",
        "        files.download(train_zip)\n",
        "    else:\n",
        "        print(\"‚úó Failed to create training results zip\")\n",
        "else:\n",
        "    print(f\"‚úó Training directory not found: {save_dir}\")\n",
        "\n",
        "# 2. Zip model files for Google Drive\n",
        "temp_dir = \"/content/esp-detection/model_export\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "files_to_zip = [\n",
        "    (espdl_path, \"Quantized model (.espdl)\"),\n",
        "    (onnx_path, \"ONNX model\"),\n",
        "    (model_path, \"PyTorch checkpoint (.pt)\")\n",
        "]\n",
        "\n",
        "print(\"\\nCollecting model files...\")\n",
        "for file_path, description in files_to_zip:\n",
        "    if os.path.exists(file_path):\n",
        "        size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
        "        print(f\"‚úì {description}: {os.path.basename(file_path)} ({size_mb:.2f} MB)\")\n",
        "        shutil.copy(file_path, temp_dir)\n",
        "    else:\n",
        "        print(f\"‚úó {description} not found: {os.path.basename(file_path)}\")\n",
        "\n",
        "# Create model files zip\n",
        "model_zip = f\"/content/esp-detection/espdet-{timestamp}.zip\"\n",
        "shutil.make_archive(model_zip.replace('.zip', ''), 'zip', temp_dir)\n",
        "\n",
        "if os.path.exists(model_zip):\n",
        "    zip_size = os.path.getsize(model_zip) / 1024 / 1024\n",
        "    print(f\"\\n‚úì Model files zip: {os.path.basename(model_zip)} ({zip_size:.2f} MB)\")\n",
        "\n",
        "    # Save to Google Drive\n",
        "    drive_zip_path = os.path.join(drive_folder, os.path.basename(model_zip))\n",
        "    shutil.copy(model_zip, drive_zip_path)\n",
        "    print(f\"‚úì Saved to Google Drive: espdet-{timestamp}/{os.path.basename(model_zip)}\")\n",
        "\n",
        "    print(\"\\nDone!\")\n",
        "else:\n",
        "    print(\"‚úó Failed to create model files zip\")"
      ],
      "metadata": {
        "id": "fV_Oov57XJiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}